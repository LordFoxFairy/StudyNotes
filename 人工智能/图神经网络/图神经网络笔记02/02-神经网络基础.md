# 神经网络基础

## 介绍

神经网络的运行过程分为三步： 前向传播、 反向传播、 参数更新， 通过不断迭代进行模型参数的更新， 以从数据中挖掘出有价值的信。

![image-20210227210221535](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201121616145.png)

1. 前向传播： 给定输入和参数， 逐层向前进行计算， 最后输出预测结果；
2. 反向传播： 基于前向传播得到的预测结果， 使用损失函数得到损失值， 然后计算
   相关参数的梯度， 该计算方法称为反向传播（back-propagation） ， 具体的细节后面将详细
   介绍；
3. 参数更新： 使用梯度下降算法对参数进行更新， 重复上述过程， 逐步迭代， 直到
   模型收敛  。

## 反向传播

以多层感知器为例子，进行介绍反向传播算法。给定样本$\{(x_n,y_n)\}_{n=1}^N$，使用多层感知器的消息传递公式可以进行前向传播，这个过程用下进行描述：

![image-20210228103424582](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201121616146.png)

给定样本$(x,y)$，前向传播得到输出$\hat y$，对应的损失值为$L(y,\hat y)$，接下来求参数矩阵$W^{(l)}$的梯度$\frac{\partial{L(y,\hat y)}}{\partial{W^{(l)}}}$，使用链式法则如下：
$$
\frac{\partial{L(y,\hat y)}}{\partial{W^{(l)}}} = \frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}}\frac{\partial{z^{(l)}}}{\partial{W^{(l)}}}
$$
定义$\frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}}$为误差项，它衡量的是对$z^{(l)}$对损失值的影响，进一步使用链式法则，可以得到：
$$
\delta^{(l)} = \frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}} = 
	\frac{\partial{a^{(l)}}}{\partial{z^{(l)}}} \times
	\frac{\partial{z^{(l+1)}}}{\partial{a^{(l)}}} \times
	\frac{\partial{L^{(y,\hat y)}}}{\partial{z^{(l+1)}}}
$$
**由于$z^{(l+1)} = W^{(l+1)}a^{(l)} + b^{(l)}$且$a^{(l)}=\sigma(z^{(l)})$进行变换可以得到：**
$$
\delta^{(l)} = \frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}} = 
	\frac{\partial{a^{(l)}}}{\partial{z^{(l)}}} \times
	\frac{\partial{z^{(l+1)}}}{\partial{a^{(l)}}} \times
	\frac{\partial{L^{(y,\hat y)}}}{\partial{z^{(l+1)}}}=\sigma^{'}(z^{(l)})\odot W^{(l+1)^T}\delta^{(l+1)}
$$
其中，$\sigma^{'}(z^{(l)})$是激活函数的导数，$\odot$表达哈达玛积，是一种对应元素相乘的二元运算符。也就是：
$$
\begin{align}
\because \delta_j^{(l)} &= \frac{\part{L{(y,\hat y)}}}{\part{z_j^{(l)}}} = \frac{\part{L{(y,\hat y)}}}{\part{a_j^{(l)}}}\cdot  \frac{\part{a_j^{(l)}}}{\part{z_j^{(l)}}}  \\

\therefore \delta^{(l)} &= \frac{\part{L{(y,\hat y)}}}{\part{a^{(l)}}}\odot  \frac{\part{a^{(l)}}}{\part{z^{(l)}}} = \nabla_a L(y,\hat y)\odot\sigma^{'}(z^{(l)})  \\
\end{align}
$$
从上可以看出，第一层的误差与第${l+1}$层的误差有关，这就是反向传播的来源。

对于$\frac{\part{L(y,\hat y)}}{\part{W^{(l)}}} \in R^{(D_l\times D_{l-1})}$，有
$$
\frac{\partial{L(y,\hat y)}}{\partial{W^{(l)}}} = \frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}}\frac{\partial{z^{(l)}}}{\partial{W^{(l)}}}=(a^{(l-1)}\delta^{(l)^T})^T 
$$
偏置项$b^{(l)}$的导数：
$$
\frac{\part{L(y,\hat y)}}{\part b^{(l)}} = \delta^{(l)}
$$

## 梯度消失

对于深度神经网络来说，虽然反向传播能够高效地计算梯度，但是随着堆叠层数和模型参数规模的增加，也给模型优化带来一些严峻的问题。

从上面的式子中可以看出，第$l$层的误差是通过第$l+1$层的误差与两层之间权重的加权，再乘以激活函数的导数得到的，如果激活函数使用`Sigmoid`，它的导数为$σ'(x)=σ(x)(1–σ(x))$，由于$σ(x)\in(0,1)$，它的导数的最大值为$σ'(x)=0.25$，当层数增加时，最后一层的误差将在前面的层中快速衰减，这会导致靠近输入层的梯度值非常小，参数几乎无法进行有效的更新，在下一次前向传播时，由于前面层的参数无法有效地从数据中获得有价值的信息供后面的层使用，模型就难以进行有效的训练。这种现象称为梯度消失。

导致梯度消失的原因在于激活函数的饱和性，比如`Sigmoid`、`Tanh`等都会带来这种问题，它们在函数值趋近于上下边界时，梯度通常比较小，再与误差项相乘将变得更小，多次的反向传播将会使得梯度值不断减小。

因此现在的神经网络通常使用`ReLU`激活函数以及其变种。

## Autograd：自动求导

`PyTorch`中，所有神经网络的核心是 `autograd` 包。先简单介绍一下这个包，然后训练第一个神经网络。

`autograd` 包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义(define-by-run）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的.

### 张量

**`torch.Tensor` 是这个包的核心类。如果设置它的属性 `.requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 `.backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性.**

**要阻止一个张量被跟踪历史，可以调用 `.detach()` 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。**

为了防止跟踪历史记录(和使用内存），可以将代码块包装在 `with torch.no_grad():` 中。在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。

还有一个类对于`autograd`的实现非常重要：`Function`。

`Tensor` 和 `Function` 互相连接生成了一个无圈图(acyclic graph)，它编码了完整的计算历史。每个张量都有一个 `.grad_fn` 属性，该属性引用了创建 `Tensor` 自身的`Function`(除非这个张量是用户手动创建的，即这个张量的 `grad_fn` 是 `None` )。

如果需要计算导数，可以在 `Tensor` 上调用 `.backward()`。如果 `Tensor` 是一个标量(即它包含一个元素的数据），则不需要为 `backward()` 指定任何参数，但是如果它有更多的元素，则需要指定一个 `gradient` 参数，该参数是形状匹配的张量。

```
import torch
import numpy as np
```

* 创建一个张量并设置requires_grad=True用来追踪其计算历史

```
x = torch.ones(2,2,requires_grad=True)
x
```

![image-20210125144530231](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201121616147.png)

* 对这个张量做一次运算：

```
y = x + 2
y
```

![image-20210125144555667](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201121616148.png)

* 对y进行更多操作

```
z = y*y*3
out = z.mean()
z,out
```

![image-20210125144657578](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201121616149.png)

* `.requires_grad_(...) `原地改变了现有张量的 `requires_grad `标志。如果没有指定的话，默认输入的这个标志是 `False`。

```
a = torch.randn(2,2)
a = ((a*3)/(a-1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a*a).sum()
print(b.grad_fn)
```

![image-20210125144714008](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201121616150.png)

### 梯度

现在开始进行反向传播，因为 `out `是一个标量，因此 `out.backward()` 和` out.backward(torch.tensor(1.)) `等价。

```
out.backward()
```

输出导数` d(out)/dx`

```
x.grad
```

![image-20210125144820261](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201121616151.png)

我们的得到的是一个数取值全部为`4.5`的矩阵。

如上可以得到，$o=\frac{1}{4}\sum_iz_i$，$z_i=3(x_i+2)^2$，因此，$\frac{\partial o}{\partial x_i}=\frac{3}{2}(x_i+2)$。

可以通过将代码块包装在 `with torch.no_grad():` 中，来阻止autograd跟踪设置了 `.requires_grad=True` 的张量的历史记录。

```
print((x**2).requires_grad)
with torch.no_grad():
    print((x**2).requires_grad)
```

![image-20210125145656102](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201121616152.png)

