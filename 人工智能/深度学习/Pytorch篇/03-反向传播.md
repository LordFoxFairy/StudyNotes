# 反向传播

## 回顾

线性模型：$\hat y=x*w$，其中$w$为权重，$x$为输入数据。

![image-20210123164445655](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216493.png)

随机梯度下降：
$$
w=w-\alpha\frac{\partial loss}{\partial w}
$$
其中，
$$
\frac{\partial loss_n}{\partial w} = 2\cdot x_n\cdot (x_n\cdot w-y_n)
$$
对于复杂模型，考虑如何计算梯度？

![image-20210123164725244](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216494.png)

## 计算图

一个两层的神经网络：
$$
\hat y = W_2(W_1\cdot X+b_1)+b_2
$$
![image-20210123165318495](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216495.png)

其中，$W_1$与$X$通过矩阵乘法得到隐藏层$H^{(1)}$。

![image-20210123165645694](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216496.png)

$H^{(1)}$和$b_1$通过加法运算得到新的一个$X$。

![image-20210123165811774](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216497.png)

神经网络的第一层，就这样计算完毕了，神经网络的第二层和运行过程和第一层一样。

![image-20210123170028670](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216498.png)

## 关于两层神经网络的问题

将各个层的计算过程进行统一，可以得到如下模型：

![image-20210123171030329](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216499.png)

如上可知，无论神经网络有多少层，都是线性的，实际只是做了一层计算。

于是为了解决这个问题，加一个非线性的激活函数，比如$sigmoid$函数，在每个线性模型后得到的线性数据，进行非线性变换。

![image-20210123171703072](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216500.png)

## 函数的复合和链式法则

> 矩阵书籍：[地址](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)

![image-20210123172254736](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216501.png)
$$
\frac{\partial f(g(x))}{\partial x} = \frac{\partial f(g(x))}{\partial g(x)}\cdot \frac{\partial g(x)}{\partial x}
$$

1. 创建计算图$(Forward)$

![image-20210123172738099](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216502.png)

2. 局部梯度

![image-20210123190454655](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216503.png)

3. 给定连续节点的梯度

![image-20210123212343416](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216504.png)

4. 用链式法则计算梯度

![image-20210123212422203](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216505.png)

![image-20210123212941634](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216506.png)

**Example：$f=x\cdot w，x=2，w=3$**

* **forward**

![image-20210123213010427](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216507.png)

* **backward**

![image-20210123213058446](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216508.png)

![image-20210123213417842](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216509.png)

![image-20210123213515866](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216510.png)

  

## 线性模型的计算图

![image-20210123214445592](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216511.png)

其中，残差项$r=\hat y-y$

## Tensor in Pytorch

![image-20210124213406916](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216512.png)

在`PyTorch`中，张量是构造动态计算图的重要组成部分。它包含`data`和`grad`，分别存储节点的值和梯度的损失。

## 用pytorch实现线性模型

如果需要自动梯度机制，则张量的元素变量`requires_grad`必须设置为`true`。

```
import torch
import numpy as np

# 数据样本
x_data = np.array([
    [1.0,2.0,3.0]
])
y_data = np.array([
    2.0,4.0,6.0
])

# 权重
w = torch.Tensor([1.0])
w.requires_grad = True

def loss(x,y,w):
    y_pred = forward(x,w)
    return (y-y_pred)**2
    
def forward(x,w):
    return x*w
```

* 定义线性模型

$$
\hat y = x \times w
$$

* 实现代码

```
print("predict(before traning)",4,forward(4,w).item())

for epoch in range(100):
    for x,y in zip(x_data,y_data):
        l = loss(x,y,w)
        l.backward()
        print("\tgrad:",x,y,w.grad.item())
        w.data = w.data - 0.01*w.grad.data
        w.grad.data.zero_() # 清空梯度
    print("progress:",epoch,l.item())
print("predict (after training)",4,forward(4,w).item())
```

* 前向传播

![image-20210125175353333](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216513.png)

* 反向传播

![image-20210125175406105](https://cdn.jsdelivr.net/gh/TheFoxFairy/ImgStg/202201070216514.png)

