{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主成分分析（PCA）\n",
    "\n",
    "## 相关背景\n",
    "\n",
    "\n",
    "在许多领域的研究与应用中，通常需要对含有多个变量的数据进行观测，收集大量数据后进行分析寻找规律。多变量大数据集无疑会为研究和应用提供丰富的信息，但是也在一定程度上增加了数据采集的工作量。更重要的是在很多情形下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性。如果分别对每个指标进行分析，分析往往是孤立的，不能完全利用数据中的信息，因此盲目减少指标会损失很多有用的信息，从而产生错误的结论。\n",
    "\n",
    "因此需要找到一种合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据降维\n",
    "\n",
    "**降维就是一种对高维度特征数据预处理方法。降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的。**\n",
    "在实际的生产和应用中，降维在一定的信息损失范围内，可以为我们节省大量的时间和成本。降维也成为应用非常广泛的数据预处理方法。\n",
    "\n",
    "优点：\n",
    "* 使得数据集更易使用。\n",
    "* 降低算法的计算开销。\n",
    "* 去除噪声。\n",
    "* 使得结果容易理解。\n",
    "\n",
    "降维的算法有很多，比如：\n",
    "* 奇异值分解(SVD)\n",
    "* 主成分分析(PCA)\n",
    "* 因子分析(FA)\n",
    "* 独立成分分析(ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA原理详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA的概念\n",
    "\n",
    "**PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。**\n",
    "\\\n",
    "\\\n",
    "PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。\n",
    "\\\n",
    "\\\n",
    "PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。\n",
    "\n",
    "\n",
    "#### 如何得到这些包含最大差异性的主成分方向？\n",
    "\n",
    "通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。\n",
    "\\\n",
    "\\\n",
    "由于得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵，所以PCA算法有两种实现方法：\n",
    "* 基于特征值分解协方差矩阵实现PCA算法\n",
    "* 基于SVD分解协方差矩阵实现PCA算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 协方差和散度矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.573026Z",
     "start_time": "2020-10-31T13:13:16.457402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6021, 0.626 , 0.032 , 0.7378, 0.0826, 0.0201, 0.2973, 0.6131,\n",
       "        0.3066, 0.3067]),\n",
       " array([0.3001, 0.1596, 0.0719, 0.7408, 0.3412, 0.0652, 0.2781, 0.184 ,\n",
       "        0.733 , 0.4353]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 设置精度\n",
    "np.set_printoptions(suppress=True, threshold=np.inf,precision=4)\n",
    "\n",
    "X = np.random.random(10)\n",
    "Y = np.random.random(10)\n",
    "X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本均值：\n",
    "\n",
    "$$\\bar x=\\frac{1}{n}\\sum_{i=1}^{N}x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.578883Z",
     "start_time": "2020-10-31T13:13:16.574003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.36243458478272184, 0.36243458478272184)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bar1 = np.mean(X)\n",
    "x_bar2 = np.sum(X)/X.shape[0]\n",
    "x_bar1,x_bar2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本方差/协方差（协方差和方差较为接近，区别在于除数为N-1）：\n",
    "$$S^2=\\frac{1}{n-1}\\sum_{i=1}^N(x_i-\\bar x)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.585716Z",
     "start_time": "2020-10-31T13:13:16.579859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(0.0718), 0.07175772310979896)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov1 = np.cov(X)\n",
    "\n",
    "x_bar = np.mean(X)\n",
    "cov2 = np.sum(np.power(X-x_bar,2))/(len(X)-1)\n",
    "\n",
    "cov1,cov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.589619Z",
     "start_time": "2020-10-31T13:13:16.586691Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.std(X) # 标准差\n",
    "# np.var(X) # 方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本X和样本Y的协方差：\n",
    "\\begin{align}\n",
    "Cov(X,Y) &= E[(X-E(X))(Y-E(Y))] \\\\\n",
    "&=\\frac{1}{n-1}\\sum_{i=1}^N(x_i-\\bar x)(y_i-\\bar y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.600354Z",
     "start_time": "2020-10-31T13:13:16.590595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.023705096386198818, 0.02370509638619881)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bar = np.mean(X)\n",
    "y_bar = np.mean(Y)\n",
    "\n",
    "xy = np.array([0. for x in X])\n",
    "for i in range(len(X)):\n",
    "    xy[i] = (X[i]-x_bar)*(Y[i]-y_bar)\n",
    "    \n",
    "cov_x_y1 = np.sum(xy)/(len(xy)-1)\n",
    "\n",
    "\n",
    "# np.cov\n",
    "xy2 = np.vstack((X,Y))\n",
    "cov_x_y2 = np.cov(xy2)\n",
    "\n",
    "cov_x_y1,cov_x_y2[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.604259Z",
     "start_time": "2020-10-31T13:13:16.601331Z"
    }
   },
   "outputs": [],
   "source": [
    "# help(np.cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于数据X的散度矩阵为$XX^T$ 。其实协方差矩阵和散度矩阵关系密切，散度矩阵就是协方差矩阵乘以（总数据量-1）。因此它们的特征值和特征向量是一样的。这里值得注意的是，散度矩阵是SVD奇异值分解的一步，因此PCA和SVD是有很大联系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.610114Z",
     "start_time": "2020-10-31T13:13:16.605235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6458195079881907, 0.6458195079881907, 0.6458195079881907)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵散度\n",
    "m = np.mean(X)\n",
    "S1 = np.sum((X-m)**2)\n",
    "\n",
    "S2 = (X-m).dot((X-m).T)\n",
    "\n",
    "S3 = np.cov(X)*(len(X)-1)\n",
    "\n",
    "S1,S2,S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征值分解矩阵原理\n",
    "\n",
    "#### 特征值与特征向量\n",
    "\n",
    "$$Av=\\lambda v$$\n",
    "其中A为矩阵，$\\lambda$为特征向量$v$对应的特征值，一个矩阵的一组特征向量是一组正交向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.615971Z",
     "start_time": "2020-10-31T13:13:16.611091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1,3,5],\n",
    "    [2,4,6],\n",
    "    [3,5,6]\n",
    "])\n",
    "#　矩阵的秩\n",
    "np.linalg.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征值分解矩阵\n",
    "对于矩阵A，有一组特征向量v，将这组向量进行正交化单位化，就能得到一组正交单位向量。**特征值分解**，就是将矩阵A分解为如下式：\n",
    "$$A=Q\\Sigma Q^{-1}$$\n",
    "\n",
    "其中，Q是矩阵A的特征向量组成的矩阵，[公式]则是一个对角阵，对角线上的元素就是特征值。\n",
    "\n",
    "参考：https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.622803Z",
     "start_time": "2020-10-31T13:13:16.617923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12.3856, -1.2572, -0.1284]),\n",
       " array([[-0.4501, -0.7706,  0.6029],\n",
       "        [-0.5881, -0.3295, -0.7368],\n",
       "        [-0.672 ,  0.5456,  0.306 ]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eigenvalue, Eigenvector = np.linalg.eig(A)\n",
    "Eigenvalue, Eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.627683Z",
     "start_time": "2020-10-31T13:13:16.623779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.3856,  0.    ,  0.    ],\n",
       "       [ 0.    , -1.2572,  0.    ],\n",
       "       [ 0.    ,  0.    , -0.1284]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角化\n",
    "Eigenvalue = np.diag(Eigenvalue)\n",
    "Eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD分解原理\n",
    "奇异值分解是一个能适用于任意矩阵的一种分解的方法，对对于任意矩阵A总是存在一个奇异值分解：\n",
    "$$A = U\\Sigma V^T$$\n",
    "\n",
    "\n",
    "$A$是一个m\\*n的矩阵，那么得到的$U$是一个m\\*m的方阵，$U$里面的正交向量被称为左奇异向量。\n",
    "$\\Sigma$是一个m\\*n的矩阵，$\\Sigma$除了对角线其它元素都为0，对角线上的元素称为奇异值。 $V^T $是$V$的转置矩阵，是一个n\\*n的矩阵，它里面的正交向量被称为右奇异值向量。而且一般来讲，会将$\\Sigma$上的值按从大到小的顺序排列。\n",
    "\n",
    "\\\n",
    "\\\n",
    "\n",
    "SVD分解矩阵A的步骤：\n",
    "\n",
    "(1) 求$AA^T$ 的特征值和特征向量，用单位化的特征向量构成 $U$。\n",
    "\n",
    "(2) 求 $A^TA$ 的特征值和特征向量，用单位化的特征向量构成 $V$。\n",
    "\n",
    "(3) 将 $AA^T$ 或者 $A^TA$ 的特征值求平方根，然后构成 $\\Sigma$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.656963Z",
     "start_time": "2020-10-31T13:13:16.644274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.3958,  0.8301],\n",
       "        [-0.7905, -0.5257],\n",
       "        [-0.4674,  0.1862]]),\n",
       " array([[14.3138,  0.    ],\n",
       "        [ 0.    ,  2.6676]]),\n",
       " array([[-0.5122, -0.8589],\n",
       "        [-0.8589,  0.5122]]),\n",
       " array([[1., 6.],\n",
       "        [7., 9.],\n",
       "        [3., 6.]]),\n",
       " array([[1, 6],\n",
       "        [7, 9],\n",
       "        [3, 6]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_by_eigen_value(EigenValues,EigenVectors):\n",
    "    \"\"\"\n",
    "    将特征值从大到小进行排序，并且基于特征值的大小，对特征向量进行排序\n",
    "    \"\"\"\n",
    "    index = np.argsort(EigenValues)[::-1]\n",
    "    EigenValues = EigenValues[index]\n",
    "    EigenVectors = EigenVectors[:,index]\n",
    "    return EigenValues,EigenVectors\n",
    "\n",
    "def svd(A):\n",
    "    ATA = np.dot(A.T,A)\n",
    "    # 求右奇异向量，V\n",
    "    lambda_V,V = np.linalg.eig(ATA)\n",
    "    lambda_V, V = sort_by_eigen_value(lambda_V, V)\n",
    "    \n",
    "    # 求奇异值\n",
    "    sigmas = lambda_V\n",
    "    \n",
    "    # 由于python里很小的数有时候是负数\n",
    "    sigmas = list(map(lambda x: np.sqrt(x)\n",
    "                      if x > 0 else 0, sigmas))\n",
    "    \n",
    "    \n",
    "    # 将末尾为0的特征值进行舍去\n",
    "    sigma_nozeros_len = len(list(filter(lambda x: x > 0,sigmas)))\n",
    "    \n",
    "    sigmas_matrix = np.diag(sigmas)\n",
    "    sigmas_matrix = sigmas_matrix[:sigma_nozeros_len,:]\n",
    "    \n",
    "    # 求左奇异向量,U\n",
    "    U = np.zeros((A.shape[0],sigma_nozeros_len))\n",
    "    for i in range(sigma_nozeros_len):\n",
    "        U[:,i] = (A.dot(V[:,i])/sigmas[i]).T\n",
    "    \n",
    "    Sigma = sigmas_matrix[:sigma_nozeros_len, :sigma_nozeros_len]\n",
    "    \n",
    "#     print(U.shape,Sigma.shape,V.shape)\n",
    "    return U,Sigma,V\n",
    "\n",
    "\n",
    "# A = np.array([[1, 1, 1, 2, 2], \n",
    "#               [0, 0, 0, 3, 3],\n",
    "#               [0, 0, 0, 1, 1], \n",
    "#               [1, 1, 1, 0, 0],\n",
    "#               [2, 2, 2, 0, 0], \n",
    "#               [5, 5, 5, 0, 0],\n",
    "#               [1, 1, 1, 0, 0]])\n",
    "\n",
    "A = np.array([\n",
    "    [1,6],\n",
    "    [7,9],\n",
    "    [3,6]\n",
    "])\n",
    "\n",
    "# 默认FULL\n",
    "U,Sigma,V = svd(A)\n",
    "U,Sigma,V,U.dot(Sigma).dot(V.T),A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.661843Z",
     "start_time": "2020-10-31T13:13:16.657939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.3958,  0.8301, -0.3928],\n",
       "        [-0.7905, -0.5257, -0.3143],\n",
       "        [-0.4674,  0.1862,  0.8642]]),\n",
       " array([14.3138,  2.6676]),\n",
       " array([[-0.5122, -0.8589],\n",
       "        [-0.8589,  0.5122]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用np.linalg.svd查看，对比\n",
    "np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.666723Z",
     "start_time": "2020-10-31T13:13:16.662819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.3958,  0.8301],\n",
       "        [-0.7905, -0.5257],\n",
       "        [-0.4674,  0.1862]]),\n",
       " array([14.3138,  2.6676]),\n",
       " array([[-0.5122, -0.8589],\n",
       "        [-0.8589,  0.5122]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full_matrices：是否进行剪切掉不需要的数据\n",
    "np.linalg.svd(A,full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA算法两种实现方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于特征值分解协方差矩阵实现PCA算法\n",
    "\n",
    "输入：数据集 $X=\\{x_1,x_2,x_3,\\cdots,x_n \\}$ ，需要降到$k$维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.696979Z",
     "start_time": "2020-10-31T13:13:16.693075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  0,  2,  0],\n",
       "       [-2,  0,  0,  1,  1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [-1,-1,0,2,0],\n",
    "    [-2,0,0,1,1]\n",
    "])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 去平均值(即去中心化)，即每一位特征减去各自的平均值。\n",
    "\n",
    "> 因为X矩阵的每行已经是零均值，所以不需要去平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.717474Z",
     "start_time": "2020-10-31T13:13:16.713570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5, -0.5,  0. ,  1.5,  0.5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取每个特征值的平均值\n",
    "np.mean(X,axis=0)# or X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.723331Z",
     "start_time": "2020-10-31T13:13:16.718450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -0.5,  0. ,  0.5, -0.5],\n",
       "       [-0.5,  0.5,  0. , -0.5,  0.5]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去中心化\n",
    "X1 = X-X.mean(axis=0)\n",
    "X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 计算协方差矩阵$\\frac{1}{n}XX^T$,注：这里除或不除样本数量n或n-1,其实对求出的特征向量没有影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.744858Z",
     "start_time": "2020-10-31T13:13:16.739978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2, 0.8],\n",
       "       [0.8, 1.2]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = X.shape[1]\n",
    "C = 1/n*X.dot(X.T)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 用特征值分解方法求协方差矩阵$\\frac{1}{n}XX^T$ 的特征值与特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.766330Z",
     "start_time": "2020-10-31T13:13:16.762426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2. , 0.4]),\n",
       " array([[ 0.7071, -0.7071],\n",
       "        [ 0.7071,  0.7071]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求协方差矩阵的特征值与特征向量\n",
    "eigenvalue, eigenvector = np.linalg.eig(C)\n",
    "eigenvalue, eigenvector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 对特征值从大到小排序，选择其中最大的$k$个。然后将其对应的$k$个特征向量分别作为行向量组成特征向量矩阵P。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.799514Z",
     "start_time": "2020-10-31T13:13:16.793657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2. , 0.4]),\n",
       " array([[ 0.7071, -0.7071],\n",
       "        [ 0.7071,  0.7071]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 2\n",
    "\n",
    "# 将特征值从大到小排序，并且基于特征值将特征向量进行排序\n",
    "index = np.argsort(eigenvalue)[::-1]\n",
    "eigenvalue = eigenvalue[index]\n",
    "eigenvector = eigenvector[:,index]\n",
    "eigenvalue,eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.804394Z",
     "start_time": "2020-10-31T13:13:16.800490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7071,  0.7071],\n",
       "       [-0.7071,  0.7071]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将其对应的 𝑘 个特征向量分别作为行向量组成特征向量矩阵P\n",
    "P = eigenvector.T[:k,:]\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 将数据转换到k个特征向量构建的新空间中，即$Y=PX$。\n",
    "> 注意：如果通过特征值分解协方差矩阵，那么只能得到一个方向的PCA降维。这个方向就是对数据矩阵X从行(或列)方向上压缩降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.829772Z",
     "start_time": "2020-10-31T13:13:16.824890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.1213, -0.7071,  0.    ,  2.1213,  0.7071])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = P[0,:].dot(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于SVD分解协方差矩阵实现PCA算法\n",
    "输入：数据集 $X=\\{x_1,x_2,x_3,\\cdots,x_n \\}$ ，需要降到$k$维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.861994Z",
     "start_time": "2020-10-31T13:13:16.856141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1],\n",
       "       [-2, -1],\n",
       "       [-3, -2],\n",
       "       [ 1,  1],\n",
       "       [ 2,  1],\n",
       "       [ 3,  2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[-1, 1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "n_samples, n_features = X.shape\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 去平均值，即每一位特征减去各自的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.882480Z",
     "start_time": "2020-10-31T13:13:16.877600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.    , 0.3333])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = X.mean(axis=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.888336Z",
     "start_time": "2020-10-31T13:13:16.883455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.    ,  0.6667],\n",
       "       [-2.    , -1.3333],\n",
       "       [-3.    , -2.3333],\n",
       "       [ 1.    ,  0.6667],\n",
       "       [ 2.    ,  0.6667],\n",
       "       [ 3.    ,  1.6667]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm = X - mean\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 通过SVD计算A的奇异值与奇异向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.907855Z",
     "start_time": "2020-10-31T13:13:16.902975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.0829,  0.8536],\n",
       "        [-0.3911, -0.0804],\n",
       "        [-0.6148, -0.3441],\n",
       "        [ 0.1955,  0.0402],\n",
       "        [ 0.3348, -0.3665],\n",
       "        [ 0.5585, -0.1028]]),\n",
       " array([[6.1406, 0.    ],\n",
       "        [0.    , 1.2754]]),\n",
       " array([[ 0.855 , -0.5187],\n",
       "        [ 0.5187,  0.855 ]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U,Sigma,V = svd(X_norm)\n",
    "U,Sigma,V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.929327Z",
     "start_time": "2020-10-31T13:13:16.922495Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6.140581853834897, array([0.855 , 0.5187])),\n",
       " (1.2753775243773604, array([-0.5187,  0.855 ]))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上述已经默认排序好了\n",
    "eig_pairs = [(np.abs(Sigma[i,i]), V[:,i]) for i in range(n_features)]\n",
    "eig_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.935185Z",
     "start_time": "2020-10-31T13:13:16.930303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.855 , 0.5187]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 1 # 确保k<n\n",
    "feature=np.array([ele[1] for ele in eig_pairs[:k]])\n",
    "feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 将数据转换到k个特征向量构建的新空间中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:16.953259Z",
     "start_time": "2020-10-31T13:13:16.949355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5092],\n",
       "       [-2.4015],\n",
       "       [-3.7752],\n",
       "       [ 1.2008],\n",
       "       [ 2.0557],\n",
       "       [ 3.4294]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=X_norm.dot(feature.T) # 将数据降到了一维\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用sklearn的PCA与手动实现的PCA做个比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:13:17.486745Z",
     "start_time": "2020-10-31T13:13:16.974734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5092]\n",
      " [ 2.4015]\n",
      " [ 3.7752]\n",
      " [-1.2008]\n",
      " [-2.0557]\n",
      " [-3.4294]]\n"
     ]
    }
   ],
   "source": [
    "##用sklearn的PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "X = np.array([[-1, 1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca=PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "print(pca.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PCA降维中，我们需要找到样本协方差矩阵$XX^T$的最大k个特征向量，然后用这最大的k个特征向量组成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵 $XX^T$,当样本数多、样本特征数也多的时候，这个计算还是很大的。当我们用到SVD分解协方差矩阵的时候，SVD有两个好处：\n",
    "\n",
    "1) 有一些SVD的实现算法可以先不求出协方差矩阵 $XX^T$ 也能求出我们的右奇异矩阵V。也就是说，我们的PCA算法可以不用做特征分解而是通过SVD来完成，这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是特征值分解。\n",
    "\n",
    "2)注意到PCA仅仅使用了我们SVD的左奇异矩阵，没有使用到右奇异值矩阵，那么右奇异值矩阵有什么用呢？\n",
    "\n",
    "假设我们的样本是m\\*n的矩阵X，如果我们通过SVD找到了矩阵 $XX^T$ 最大的k个特征向量组成的k\\*n的矩阵 $V^T$,则我们可以做如下处理：\n",
    "\n",
    "$$X^{'}_{m*k}=X_{m*n}V^T_{n*k}$$\n",
    "\n",
    "可以得到一个m\\*k的矩阵X',这个矩阵和我们原来m\\*n的矩阵X相比，列数从n减到了k，可见对列数进行了压缩。也就是说，左奇异矩阵可以用于对行数的压缩；右奇异矩阵可以用于对列(即特征维度)的压缩。这就是我们用SVD分解协方差矩阵实现PCA可以得到两个方向的PCA降维(即行和列两个方向)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA的理论推导\n",
    "\n",
    "PCA有两种通俗易懂的解释：\n",
    "* 最大方差理论；\n",
    "* 最小化降维造成的损失。\n",
    "\n",
    "扩:<a href=\"https://blog.csdn.net/Dark_Scope/article/details/53150883\">从PCA和SVD的关系拾遗</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择降维后的维度K(主成分的个数)\n",
    "\n",
    "如何选择主成分个数K呢？先来定义两个概念：\n",
    "* avaerage squared projection error:$\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}-x^{(i)}_{appror}||^2$，其中$x^{(i)}_{appror}$为映射值\n",
    "* total variation in the data:$\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2$\n",
    "\n",
    "选择不同的K值，然后用下面的式子不断计算，选取能够满足下列式子条件的最小K值即可。\n",
    "$$\\frac{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}-x^{(i)}_{appror}||^2}{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2}$$\n",
    "其中t值可以由自己定，比如t值取0.01，则代表了该PCA算法保留了99%的主要信息。当觉得误差需要更小，可以把t值设置的更小。上式还可以用SVD分解时产生的S矩阵来表示，如下面的式子：\n",
    "$$1-\\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}} \\lt = t$$\n",
    "\n",
    "\n",
    "扩:<a href=\"https://www.cnblogs.com/zy230530/p/7074215.html\">机器学习实战之PCA</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "PCA - 主成分分析",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "190.694px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
