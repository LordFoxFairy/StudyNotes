{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率潜在语义分析\n",
    "\n",
    "**概率潜在语义分析（PLSA）也称为概率潜在语义索引（PLSI）**，是一种利用概率生成模型对文本集合进行话题分析的无监督学习方法。\n",
    "\n",
    "模型的最大特点是用**隐变量**表示**话题**；整个模型表示**文本生成话题**，话题生成单词，从而得到**单词-文本**共线数据的过程；假设每个文本由一个话题分布决定，每个话题由一个单词分布决定。\n",
    "\n",
    "**概率潜在语义分析**受**潜在语义分析**的启发，在1999年由**Hofmann**提出，前者基于概率模型，后者基于非概率模型。**概率潜在语义分析**最初用于**文本数据挖掘**，后来扩展到其他的领域。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率潜在语义分析模型\n",
    "\n",
    "### 基本想法\n",
    "\n",
    "给定一个文本集合，每个文本讨论若干个话题，每个话题由若干个单词表示。对文本集合进行概率潜在语义分析，就能够发现每个文本的话题，以及每个话题的单词。话题是不能从数据中直接观察到的，是潜在的。\n",
    "\n",
    "文本集合转换为文本-单词共现数据，具体表现为单词-文本矩阵：\n",
    "\n",
    "![avatar](./images/18-1.png)\n",
    "\n",
    "其中，每一行对应一个单词，每一列对应一个文本，每一个元素表示单词在文本中出现的次数。\n",
    "\n",
    "文本数据基于如下的概率模型产生（共现模型）：首先有话题的概率分布，然后有话题给定条件下文本的条件概率分布，以及话题给定条件下单词的条件概率分布。概率潜在语义分析就是发现由隐变量表示的话题，其学习策略是观测数据的极大似然估计，其学习算法是EM算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成模型\n",
    "假设有单词集合$W=\\{w_1,w_2,\\cdots,w_M\\}$，其中$M$是单词个数；文本（指标）集合$D={d_1,d_2,\\cdots,d_N}$，其中$N$是文本个数；话题集合$Z=\\{z_1,z_2,\\cdots,z_K\\}$，其中$K$是预先设定的话题个数。随机变量$w$取值于单词集合；随机变量$d$取值于文本集合，随机变量$z$取值于话题集合。概率分布$P(d)$、条件概率分布$P(z|d)$、条件概率分布$P(w|z)$皆属于多项分布，其中$P(d)$表示生产文本d的概率，$P(z|d)$表示文本d生成话题z的概率，$P(w|z)$表示话题z生成单词w的概率。\n",
    "\n",
    "每个文本d拥有自己的话题概率分布P(z|d)，每个话题z拥有自己的单词概率分布P(w|z)；也就是说一个文本的内容由其相关话题决定，一个话题的内容由其相关单词决定。\n",
    "\n",
    "生成模型通过以下步骤生成文本-单词共现数据：\n",
    "1. 依据概率分布P(d)，从文本（指标）集合中随机选取一个文本d，共生成N个文本；针对每个文本，执行以下操作；\n",
    "2. 在文本d给定条件下，依据条件概率分布P(z|d)，从话题集合随机选取一个话题z，共生成L个话题，这里L是文本长度；\n",
    "3. 在话题z给定条件下，依据条件概率分布P(w|z)，从单词集合中随机选取一个单词w。\n",
    "\n",
    "生成模型中， 单词变量 $w$ 与文本变量 $d$ 是观测变量， 话题变量 $z$ 是隐变量， 也就是说模型生成的是单词-话题-文本三元组合 ($w, z ,d$)的集合， 但观测到的单词-文本二元组 （$w, d$）的集合， 观测数据表示为单词-文本矩阵 $T$的形式，矩阵 $T$ 的行表示单词，列表示文本， 元素表示单词-文本对（$w, d$）的出现次数。  \n",
    "\n",
    "从数据的生成过程可以推出，文本-单词共现数据$T$的生成概率为所有单词-文本对($w,d$)的生成概率的乘积：  \n",
    "\n",
    "$$P(T) = \\prod_{w,d}P(w,d)^{n(w,d)}$$\n",
    "\n",
    "这里 $n(w,d)$ 表示 ($w,d$)的出现次数，单词-文本对出现的总次数是 $N\\times L$。 每个单词-文本对（$w,d$）的生成概率由一下公式决定：  \n",
    "\n",
    "\n",
    "$$P ( w , d ) = P ( d ) P ( w | d ) = P ( d ) \\sum _ { z } P ( w,z|d )= P ( d ) \\sum _ { z } P ( z | d ) P ( w | z )$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共现模型\n",
    "\n",
    "共现模型定义：\n",
    "\n",
    "$$P(w,d) = \\sum_{z\\in Z} P(z)P(w|z)P(d|z)$$\n",
    "\n",
    "虽然生成模型与共现模型在概率公式意义上是等价的，但是拥有不同的性质。生成模型刻画文本-单词共现数据生成的过程，共现模型描述文本-单词共现数据拥有的模式。  \n",
    "\n",
    "\n",
    "### 模型性质\n",
    "\n",
    "如果直接定义单词与文本的共现概率 $P(w,d)$，模型参数的个数是 $O(M\\times N)$，其中 $M$ 是单词数， $N$ 是文本数。 概率潜在语义分析的生成模型和共现模型的参数个数是 $O(M\\times K + N\\times K)$，其中 $K$ 是话题数。 现实中 $K<<M$, 所以**概率潜在语义分析通过话题对数据进行了更简洁的表示，减少了学习过程中过拟合的可能性**。\n",
    "\n",
    "模型中的概率分布 𝑃(𝑤|𝑑) 可以由参数空间中的单纯形表示。 𝑀 维参数空间中，单词单纯形表示所有可能的文本的分布，在其中的话题单纯形表示在 𝐾 个话题定义下的所有可能的文本的分布。话题单纯形是单词单纯形的子集，表示潜在语义空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率潜在语义分析的算法\n",
    "\n",
    "概率潜在语义分析的学习通常采用EM算法通过迭代学习模型的参数，$P ( w | z )$和$P ( z| d )$，而$P（d）$可直接统计得出。\n",
    "\n",
    "输入： 设单词集合为 $W = ${$w_{1}, w_{2},..., w_{M}$}, 文本集合为 $D=${$d_{1}, d_{2},..., d_{N}$}, 话题集合为 $Z=${$z_{1}, z_{2},..., z_{K}$}, 共现数据 $\\left \\{ n(w_{i}, d_{j}) \\right \\}, i = 1,2,..., M, j = 1,2,...,N;$  \n",
    "\n",
    "输出： $P(w_{i}|z_{k})$ 和 $P(z_{k}|d_{j})$.\n",
    "\n",
    "1. 设置参数 $P(w_{i}|z_{k})$ 和 $P(z_{k}|d_{j})$ 的初始值。\n",
    "\n",
    "2. 迭代执行以下E步，M步，直到收敛为止。  \n",
    "\n",
    " E步：  \n",
    "    $P(z_{k}|w_{i},d_{j})=\\frac{P(w_{i}|z_{k})P(z_{k}|d_{j})}{\\sum_{k=1}^{K}P(w_{i}|z_{k})P(z_{k}|d_{j})}$  \n",
    "  \n",
    " M步：  \n",
    "    $P(w_{i}|z_{k})=\\frac{\\sum_{j=1}^{N}n(w_{i},d_{j})P(z_{k}|w_{i},d_{j})}{\\sum_{m=1}^{M}\\sum_{j=1}^{N}n(w_{m},d_{j})P(z_{k}|w_{m},d_{j})}$  \n",
    "    \n",
    "    $P(z_{k}|d_{j}) = \\frac{\\sum_{i=1}^{M}n(w_{i},d_{j})P(z_{k}|w_{i},d_{j})}{n(d_{j})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
